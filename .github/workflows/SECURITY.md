# ğŸ”’ Security Tools for AI/ML Projects

Ce projet utilise une **stack de sÃ©curitÃ© complÃ¨te** adaptÃ©e aux projets d'IA/ML.

---

## ğŸ“‹ Outils de sÃ©curitÃ© intÃ©grÃ©s

### 1. **Secrets Detection** ğŸ”‘

**Outils:**
- **Gitleaks** - DÃ©tecte les secrets dans le code (API keys, tokens, passwords)
- **TruffleHog** - Scan des secrets avec vÃ©rification

**DÃ©tecte:**
- âœ… API keys OpenAI, Anthropic, HuggingFace
- âœ… AWS/GCP/Azure credentials
- âœ… Database passwords
- âœ… Private keys
- âœ… Tokens dans l'historique Git

**Quand:** Sur chaque PR

---

### 2. **SAST (Static Application Security Testing)** ğŸ”

**Outils:**
- **Bandit** - Security linter Python
- **Semgrep** - Pattern matching avec rÃ¨gles AI/ML

**DÃ©tecte:**
- âœ… SQL injection
- âœ… Hardcoded secrets
- âœ… Unsafe deserialization (pickle files)
- âœ… Path traversal
- âœ… YAML/JSON injection
- âœ… Unsafe file operations

**RÃ¨gles spÃ©cifiques IA:**
- Chargement de modÃ¨les non vÃ©rifiÃ©s
- DÃ©sÃ©rialisation de donnÃ©es ML dangereuses
- ExÃ©cution de code dynamique

**Quand:** Sur chaque PR

---

### 3. **Dependency Security** ğŸ“¦

**Outils:**
- **Safety** - VÃ©rifie les vulnÃ©rabilitÃ©s PyPI
- **Pip-audit** - Alternative Ã  Safety
- **Snyk** - Scan des dÃ©pendances avec base de donnÃ©es complÃ¨te

**VÃ©rifie:**
- âœ… torch, transformers, tensorflow
- âœ… langchain, openai, anthropic
- âœ… numpy, scipy, pandas
- âœ… Toutes les dÃ©pendances transitives

**Pourquoi important pour l'IA:**
- Les librairies ML ont souvent des vulnÃ©rabilitÃ©s critiques
- Supply chain attacks sur des modÃ¨les prÃ©-entraÃ®nÃ©s
- Backdoors dans les poids de modÃ¨les

**Quand:** Sur chaque PR + release

---

### 4. **Supply Chain Security** ğŸ”—

**Outils:**
- **Dependency Review** - GitHub native
- **SBOM Generation** - Software Bill of Materials (CycloneDX)

**GÃ©nÃ¨re:**
- âœ… Liste complÃ¨te des dÃ©pendances
- âœ… Versions exactes
- âœ… Licences
- âœ… Hashes de vÃ©rification

**UtilitÃ©:**
- TraÃ§abilitÃ© complÃ¨te
- Audit de conformitÃ©
- DÃ©tection de tampering

**Quand:** Sur chaque PR + release

---

### 5. **License Compliance** âš–ï¸

**Outils:**
- **pip-licenses** - Extraction des licences

**VÃ©rifie:**
- âŒ Bloque GPL, AGPL, LGPL (copyleft)
- âœ… Autorise MIT, Apache, BSD

**Pourquoi critique pour l'IA:**
- Beaucoup de modÃ¨les ML ont des licences restrictives
- HuggingFace models peuvent Ãªtre non-commerciales
- Evite les problÃ¨mes lÃ©gaux

**Quand:** Sur chaque PR

---

### 6. **CodeQL (Advanced SAST)** ğŸ§ 

**Outil:**
- **GitHub CodeQL** - Analyse sÃ©mantique du code

**Analyse:**
- âœ… Data flow analysis
- âœ… Taint tracking
- âœ… Control flow analysis
- âœ… Security patterns

**Queries:**
- `security-extended` - VulnÃ©rabilitÃ©s Ã©tendues
- `security-and-quality` - QualitÃ© + sÃ©curitÃ©

**Quand:** Sur chaque PR

---

### 7. **Trivy (Vulnerability Scanner)** ğŸ›¡ï¸

**Outil:**
- **Aqua Trivy** - Scanner universel

**Scanne:**
- âœ… Filesystem
- âœ… Dependencies Python
- âœ… OS packages
- âœ… Containers (si applicable)

**SÃ©vÃ©ritÃ©:**
- CRITICAL, HIGH, MEDIUM

**Quand:** Sur chaque PR + release

---

## ğŸš¨ Cas d'usage spÃ©cifiques IA/ML

### Risque 1: Model Poisoning
**Outil:** Bandit + Semgrep
**DÃ©tecte:** Chargement de modÃ¨les non vÃ©rifiÃ©s
```python
# âŒ Dangereux
model = torch.load("model.pth")  # DÃ©tectÃ© par Bandit

# âœ… SÃ»r
model = torch.load("model.pth", map_location="cpu", weights_only=True)
```

### Risque 2: Data Exfiltration
**Outil:** Semgrep + CodeQL
**DÃ©tecte:** Envoi de donnÃ©es sensibles
```python
# âŒ Dangereux - DÃ©tectÃ©
requests.post(UNKNOWN_URL, data=user_data)

# âœ… SÃ»r - Whitelisted URLs only
```

### Risque 3: Pickle Deserialization
**Outil:** Bandit
**DÃ©tecte:** Utilisation non sÃ©curisÃ©e de pickle
```python
# âŒ Dangereux
import pickle
data = pickle.load(file)  # DÃ©tectÃ©

# âœ… SÃ»r
import json
data = json.load(file)
```

### Risque 4: API Key Leaks
**Outil:** Gitleaks + TruffleHog
**DÃ©tecte:** Tokens dans le code
```python
# âŒ Dangereux - DÃ©tectÃ© immÃ©diatement
OPENAI_API_KEY = "sk-proj-abc123..."

# âœ… SÃ»r
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
```

---

## ğŸ”§ Configuration requise

### Secrets GitHub (optionnels)

**SNYK_TOKEN** (recommandÃ©):
1. CrÃ©er compte sur https://snyk.io
2. GÃ©nÃ©rer token API
3. Ajouter dans GitHub Secrets

**Sans token Snyk:**
Les autres outils fonctionnent sans configuration!

---

## ğŸ“Š Rapports gÃ©nÃ©rÃ©s

Chaque PR gÃ©nÃ¨re:
- ğŸ“„ Bandit JSON report
- ğŸ“„ Semgrep JSON report
- ğŸ“„ Safety JSON report
- ğŸ“„ Pip-audit JSON report
- ğŸ“„ SBOM (CycloneDX JSON)
- ğŸ“„ Licenses JSON/Markdown
- ğŸ“„ Trivy SARIF

**AccÃ¨s:**
GitHub Actions â†’ Artifacts

---

## âœ… Best Practices

### Pour les contributeurs:

1. **Avant de commit:**
   ```bash
   # Scan local
   gitleaks detect --source .
   bandit -r src/
   ```

2. **Tester les dÃ©pendances:**
   ```bash
   safety check
   pip-audit
   ```

3. **VÃ©rifier les licences:**
   ```bash
   pip-licenses --fail-on="GPL;AGPL"
   ```

### Pour l'admin:

1. **Review Security tab** sur GitHub rÃ©guliÃ¨rement
2. **VÃ©rifier les Dependabot alerts**
3. **Auditer le SBOM** avant chaque release
4. **Valider les licences** des nouvelles dÃ©pendances

---

## ğŸ†˜ En cas d'alerte

### VulnÃ©rabilitÃ© CRITICAL trouvÃ©e:

1. **Ne pas merger la PR**
2. **Identifier la dÃ©pendance:** Regarder le rapport
3. **Chercher un patch:**
   ```bash
   pip install --upgrade <package>
   ```
4. **Si pas de patch:** Trouver une alternative

### Secret dÃ©tectÃ©:

1. **STOP immÃ©diatement**
2. **RÃ©voquer le secret** (OpenAI, AWS, etc.)
3. **Supprimer de l'historique:**
   ```bash
   git filter-branch --force --index-filter \
     'git rm --cached --ignore-unmatch <file>' HEAD
   ```
4. **Forcer un nouveau secret**

### License non-compatible:

1. **Identifier la dÃ©pendance**
2. **Chercher une alternative** avec licence compatible
3. **Ou nÃ©gocier** une licence commerciale

---

## ğŸ“š Ressources

- [OWASP Top 10 for LLM](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework)
- [HuggingFace Model Cards](https://huggingface.co/docs/hub/model-cards)
- [Microsoft AI Security Best Practices](https://www.microsoft.com/en-us/security/business/ai-machine-learning)

---

## ğŸ¯ RÃ©sumÃ©

| Risque | Outil | Quand |
|--------|-------|-------|
| Secrets in code | Gitleaks, TruffleHog | Chaque PR |
| Code vulnerabilities | Bandit, Semgrep, CodeQL | Chaque PR |
| Dependency CVEs | Safety, Pip-audit, Snyk | Chaque PR |
| Supply chain | SBOM, Dependency Review | Chaque PR |
| License issues | pip-licenses | Chaque PR |
| Container vulns | Trivy | Chaque PR + Release |

**RÃ©sultat:** Stack de sÃ©curitÃ© de niveau **entreprise** pour projet IA! ğŸ›¡ï¸
